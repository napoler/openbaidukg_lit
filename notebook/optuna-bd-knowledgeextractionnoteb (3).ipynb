{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T14:24:40.435015Z","iopub.execute_input":"2021-06-23T14:24:40.435383Z","iopub.status.idle":"2021-06-23T14:24:40.778919Z","shell.execute_reply.started":"2021-06-23T14:24:40.435307Z","shell.execute_reply":"2021-06-23T14:24:40.778128Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/optuna-bd-knowledgeextractionnoteb/model.bin\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/__results__.html\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/__notebook__.ipynb\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/__output__.json\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/custom.css\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/checkpoints/epoch=6-step=28455.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/checkpoints/epoch=5-step=27316.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/run-30a081bw.wandb\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/logs/debug.log\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/logs/debug-internal.log\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/files/wandb-summary.json\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/files/config.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/files/output.log\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/files/requirements.txt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/files/wandb-metadata.json\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/wandb/run-20210622_142003-30a081bw/tmp/code/kaggle.ipynb\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_19/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_19/events.out.tfevents.1624371357.8a1032e0b20d.24.19\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_19/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_8/events.out.tfevents.1624371272.8a1032e0b20d.24.8\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_8/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_8/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_35/events.out.tfevents.1624371481.8a1032e0b20d.24.35\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_35/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_35/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_25/events.out.tfevents.1624371404.8a1032e0b20d.24.25\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_25/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_25/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_0/events.out.tfevents.1624371207.8a1032e0b20d.24.0\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_0/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_0/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_12/events.out.tfevents.1624371303.8a1032e0b20d.24.12\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_12/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_12/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_42/events.out.tfevents.1624371536.8a1032e0b20d.24.42\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_42/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_42/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_38/events.out.tfevents.1624371505.8a1032e0b20d.24.38\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_38/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_38/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_40/events.out.tfevents.1624371520.8a1032e0b20d.24.40\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_40/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_40/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_15/events.out.tfevents.1624371326.8a1032e0b20d.24.15\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_15/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_15/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_48/events.out.tfevents.1624371583.8a1032e0b20d.24.48\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_48/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_48/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_26/events.out.tfevents.1624371411.8a1032e0b20d.24.26\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_26/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_26/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_43/events.out.tfevents.1624371544.8a1032e0b20d.24.43\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_43/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_43/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_31/events.out.tfevents.1624371450.8a1032e0b20d.24.31\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_31/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_31/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_18/events.out.tfevents.1624371350.8a1032e0b20d.24.18\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_18/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_18/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_30/events.out.tfevents.1624371443.8a1032e0b20d.24.30\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_30/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_30/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_41/events.out.tfevents.1624371528.8a1032e0b20d.24.41\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_41/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_41/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_28/events.out.tfevents.1624371427.8a1032e0b20d.24.28\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_28/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_28/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_21/events.out.tfevents.1624371373.8a1032e0b20d.24.21\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_21/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_21/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_9/events.out.tfevents.1624371280.8a1032e0b20d.24.9\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_9/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_9/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_44/events.out.tfevents.1624371551.8a1032e0b20d.24.44\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_44/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_44/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_11/events.out.tfevents.1624371295.8a1032e0b20d.24.11\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_11/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_11/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_16/events.out.tfevents.1624371334.8a1032e0b20d.24.16\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_16/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_16/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_33/events.out.tfevents.1624371466.8a1032e0b20d.24.33\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_33/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_33/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_49/events.out.tfevents.1624371591.8a1032e0b20d.24.49\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_49/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_49/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_13/events.out.tfevents.1624371311.8a1032e0b20d.24.13\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_13/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_13/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_27/events.out.tfevents.1624371419.8a1032e0b20d.24.27\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_27/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_27/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_17/events.out.tfevents.1624371342.8a1032e0b20d.24.17\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_17/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_17/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_47/events.out.tfevents.1624371575.8a1032e0b20d.24.47\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_47/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_47/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_39/events.out.tfevents.1624371512.8a1032e0b20d.24.39\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_39/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_39/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_23/events.out.tfevents.1624371388.8a1032e0b20d.24.23\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_23/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_23/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_46/events.out.tfevents.1624371567.8a1032e0b20d.24.46\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_46/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_46/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_2/events.out.tfevents.1624371225.8a1032e0b20d.24.2\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_2/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_2/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_20/events.out.tfevents.1624371365.8a1032e0b20d.24.20\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_20/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_20/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_45/events.out.tfevents.1624371559.8a1032e0b20d.24.45\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_45/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_45/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_14/events.out.tfevents.1624371318.8a1032e0b20d.24.14\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_14/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_14/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_24/events.out.tfevents.1624371396.8a1032e0b20d.24.24\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_24/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_24/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_29/events.out.tfevents.1624371435.8a1032e0b20d.24.29\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_29/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_29/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_36/events.out.tfevents.1624371489.8a1032e0b20d.24.36\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_36/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_36/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_32/events.out.tfevents.1624371458.8a1032e0b20d.24.32\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_32/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_32/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_10/events.out.tfevents.1624371287.8a1032e0b20d.24.10\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_10/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_10/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_1/events.out.tfevents.1624371217.8a1032e0b20d.24.1\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_1/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_1/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_6/events.out.tfevents.1624371256.8a1032e0b20d.24.6\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_6/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_6/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_4/events.out.tfevents.1624371241.8a1032e0b20d.24.4\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_4/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_4/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_3/events.out.tfevents.1624371233.8a1032e0b20d.24.3\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_3/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_3/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_5/events.out.tfevents.1624371249.8a1032e0b20d.24.5\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_5/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_5/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_34/events.out.tfevents.1624371473.8a1032e0b20d.24.34\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_34/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_34/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_37/events.out.tfevents.1624371497.8a1032e0b20d.24.37\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_37/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_37/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_7/events.out.tfevents.1624371264.8a1032e0b20d.24.7\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_7/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_7/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_22/events.out.tfevents.1624371381.8a1032e0b20d.24.22\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_22/hparams.yaml\n/kaggle/input/optuna-bd-knowledgeextractionnoteb/lightning_logs/version_22/checkpoints/epoch=0-step=6.ckpt\n/kaggle/input/openkgmodelml/model.bin\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/run-3cu3oiy2.wandb\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/logs/debug.log\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/logs/debug-internal.log\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/wandb-summary.json\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/config.yaml\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/output.log\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/requirements.txt\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/wandb-metadata.json\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/百度数据做开放关系BD_Knowledge Extractionnotebookb9f0237b84/3cu3oiy2/checkpoints/chinese-out.ckpt\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/百度数据做开放关系BD_Knowledge Extractionnotebookb9f0237b84/3cu3oiy2/checkpoints/chinese-out-v1.ckpt\n/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/tmp/code/kaggle.ipynb\n/kaggle/input/bdknowledgeextractionopenkg/train.bin\n/kaggle/input/bdknowledgeextractionopenkg/data.pkt\n/kaggle/input/bdknowledgeextractionopenkg/test.bin\n/kaggle/input/bdknowledgeextractionopenkg/seq2seqtrian.pkt\n/kaggle/input/bdknowledgeextractionopenkg/seq2seqdev.pkt\n/kaggle/input/bdknowledgeextractionopenkg/dev.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers optuna\n# !pip install performer-pytorch\n!pip install pytorch-lightning wandb -q\n# !pip install tkit_mlp_pytorch\nos.environ[\"WANDB_SILENT\"] = \"true\"\nos.environ[\"WANDB_API_KEY\"] = \"ea464ba3c9768e43f54698ec8b8837c5989d4fcf\"","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:24:40.780309Z","iopub.execute_input":"2021-06-23T14:24:40.780668Z","iopub.status.idle":"2021-06-23T14:24:53.517232Z","shell.execute_reply.started":"2021-06-23T14:24:40.780632Z","shell.execute_reply":"2021-06-23T14:24:53.516148Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.5.1)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (2.7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.19.5)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.8.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.59.0)\nRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna) (1.5.8)\nRequirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.5.4)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (20.9)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (5.0.1)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna) (3.7.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (2.4.7)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (3.4.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (1.0.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.3.17)\nRequirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.0.4)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (2.8.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.1.4)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.3.0)\nRequirement already satisfied: PyYAML>=3.12 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.3.1)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (2.1.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (1.5.0)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.5.1)\nRequirement already satisfied: colorama>=0.3.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.4.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna) (1.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil->alembic->optuna) (1.15.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer,AdamW\n# from torch.utils.data import \n# from performer_pytorch import PerformerLM\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\n# from torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader, random_split,TensorDataset\nimport pytorch_lightning as pl\n# from tkit_mlp_pytorch import MLP\nfrom transformers import BertTokenizer, BertForMaskedLM,AlbertTokenizer,BertConfig\n# import torch\\\n\nimport optuna\nfrom optuna.integration import PyTorchLightningPruningCallback\n\n# help(DataLoader)\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\n# wandb_logger = WandbLogger(project=\"百度数据做开放关系BD_Knowledge Extractionnotebookb9f0237b84\")\n# /kaggle/input/reformerchinesemodel/epoch4step21209.ckpt\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint,LearningRateMonitor\n# 自动停止\n# https://pytorch-lightning.readthedocs.io/en/1.2.1/common/early_stopping.html\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nimport torch.optim as optim\n\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice\n\n\n\n\n# 解决不显示图表问题\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\ninit_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:24:53.519404Z","iopub.execute_input":"2021-06-23T14:24:53.519785Z","iopub.status.idle":"2021-06-23T14:25:02.651968Z","shell.execute_reply.started":"2021-06-23T14:24:53.519745Z","shell.execute_reply":"2021-06-23T14:25:02.651131Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"cell_type":"code","source":"\n# https://huggingface.co/uer/roberta-base-word-chinese-cluecorpussmall\n\n# PATH=\"hfl/chinese-bert-wwm-ext\"\n# PATH=\"hfl/chinese-roberta-wwm-ext\"\nPATHV='uer/roberta-base-word-chinese-cluecorpussmall'\n# PATH=\"bert-base-chinese\"\n# tokenizer = BertTokenizer.from_pretrained(\"uer/roberta-base-word-chinese-cluecorpussmall\")\n# tokenizer = AlbertTokenizer.from_pretrained(\"uer/roberta-base-word-chinese-cluecorpussmall\")\n\n# model = BertForMaskedLM.from_pretrained(\"uer/roberta-base-word-chinese-cluecorpussmall\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:02.653731Z","iopub.execute_input":"2021-06-23T14:25:02.654089Z","iopub.status.idle":"2021-06-23T14:25:02.658199Z","shell.execute_reply.started":"2021-06-23T14:25:02.654050Z","shell.execute_reply":"2021-06-23T14:25:02.657077Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:02.659540Z","iopub.execute_input":"2021-06-23T14:25:02.659867Z","iopub.status.idle":"2021-06-23T14:25:02.673091Z","shell.execute_reply.started":"2021-06-23T14:25:02.659833Z","shell.execute_reply":"2021-06-23T14:25:02.672160Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dev=torch.load(\"../input/bdknowledgeextractionopenkg/dev.bin\")\ntest=torch.load(\"../input/bdknowledgeextractionopenkg/test.bin\")\ntrain=torch.load(\"../input/bdknowledgeextractionopenkg/train.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:02.674419Z","iopub.execute_input":"2021-06-23T14:25:02.674775Z","iopub.status.idle":"2021-06-23T14:25:23.127375Z","shell.execute_reply.started":"2021-06-23T14:25:02.674743Z","shell.execute_reply":"2021-06-23T14:25:23.126547Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"devdataset=TensorDataset(dev['input_ids'],dev['token_type_ids'],dev['attention_mask'],dev['labels'])\ntestdataset=TensorDataset(test['input_ids'],test['token_type_ids'],test['attention_mask'],test['labels'])\ntraindataset=TensorDataset(train['input_ids'],train['token_type_ids'],train['attention_mask'],train['labels'])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.128722Z","iopub.execute_input":"2021-06-23T14:25:23.129072Z","iopub.status.idle":"2021-06-23T14:25:23.136688Z","shell.execute_reply.started":"2021-06-23T14:25:23.129036Z","shell.execute_reply":"2021-06-23T14:25:23.135741Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 小数据截取测试用","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# batch_size=80\n\n# batch_size=72\n\n\n# train_loader=DataLoader(traindataset,batch_size=batch_size, shuffle=True, )\n# val_loader=DataLoader(devdataset,batch_size=batch_size, shuffle=False, )\n# test_loader=DataLoader(testdataset,batch_size=batch_size, shuffle=False, )","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.139935Z","iopub.execute_input":"2021-06-23T14:25:23.140286Z","iopub.status.idle":"2021-06-23T14:25:23.149745Z","shell.execute_reply.started":"2021-06-23T14:25:23.140251Z","shell.execute_reply":"2021-06-23T14:25:23.148796Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class LitAutoLM(pl.LightningModule):\n    \"\"\"\n    继承自bertlm模型\n    做预测\n    \"\"\"\n    def __init__(self, learning_rate=3e-4,T_max=500,optimizer_name=\"AdamW\", **kwargs):\n        super().__init__()\n        self.hparams.config = BertConfig.from_pretrained(\"uer/roberta-small-word-chinese-cluecorpussmall\")\n        \n        self.save_hyperparameters()\n#         self.tokenizer = AlbertTokenizer.from_pretrained(\"uer/roberta-base-word-chinese-cluecorpussmall\")\n#         self.model = BertForMaskedLM.from_pretrained(\"uer/roberta-base-word-chinese-cluecorpussmall\")\n# https://huggingface.co/uer/roberta-tiny-word-chinese-cluecorpussmall\n#         configuration = BertConfig.from_pretrained(\"uer/roberta-tiny-word-chinese-cluecorpussmall\")\n\n        self.tokenizer = AlbertTokenizer.from_pretrained(\"uer/roberta-tiny-word-chinese-cluecorpussmall\")\n        self.model = BertForMaskedLM.from_pretrained(\"uer/roberta-small-word-chinese-cluecorpussmall\",config=self.hparams.config)\n\n#         self.loss_fn = torch.nn.MSELoss()\n#         tokenizer.save_vocabulary(\"./\")\n    def forward(self, input_ids,token_type_ids=None,attention_mask=None,labels=None):\n        # in lightning, forward defines the prediction/inference actions\n        outputs = self.model(input_ids=input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask,labels=labels)\n#         loss = outputs.loss\n#         logits = outputs.logits\n        return outputs\n    def training_step(self, batch, batch_idx):\n        # training_step defined the train loop.\n        # It is independent of forward\n        x, token_type_ids,attention_mask,y = batch\n        outputs = self(x, token_type_ids,attention_mask,y)\n        self.log('train_loss', outputs.loss)\n        return outputs.loss\n    def validation_step(self, batch, batch_idx):\n        # training_step defined the train loop.\n        # It is independent of forward\n        x, token_type_ids,attention_mask,y = batch\n        outputs = self(x, token_type_ids,attention_mask,y)\n        self.log('val_loss', outputs.loss)\n        self.log(\"hp_metric\",outputs.loss) # 这个参数用于参数效果对比\n        return outputs.loss\n\n    def test_step(self, batch, batch_idx):\n        # training_step defined the train loop.\n        # It is independent of forward\n        x, token_type_ids,attention_mask,y = batch\n        outputs = self(x, token_type_ids,attention_mask,y)\n        self.log('test_loss', outputs.loss)\n        \n        \n        logits = outputs.logits\n        for one,one2 in zip(logits.argmax(dim=-1).tolist(),y.tolist()):\n            kgtext=[]\n            kgtext2=[]\n            for it,it2 in zip(one,one2):\n    #             print(tokenizer.decode(it))\n#                 if tokenizer.decode(it) in [\"[SEP]\",\"[PAD]\"]:\n#                     continue\n                kgtext.append(tokenizer.decode(it))\n                kgtext.append(tokenizer.decode(it2))\n            print(kgtext)\n            \n            \n        return outputs.loss\n    def pred(self, text,ner1,ner2):\n        # training_step defined the train loop.\n        # It is independent of forward\n        inputs = self.tokenizer([ner1+\"和\"+ner2+\"关系为\"+\"\".join([\"[MASK]\"]*10)],[text], return_tensors=\"pt\", padding=\"max_length\",max_length=128,truncation=True)\n        outputs = self(**inputs)\n        logits = outputs.logits\n        kgtext=[]\n        full=[]\n        words=tokenizer.tokenize(text)\n        for it in logits.argmax(dim=-1).tolist()[0]:\n#             print(tokenizer.decode(it))\n            full.append(tokenizer.decode(it))\n            if tokenizer.decode(it) in [\"[SEP]\",\"[PAD]\"]:\n                continue\n            \n            kgtext.append(tokenizer.decode(it))\n#         i=\"\".join(kgtext).index('关系为')\n#         return kgtext[i+3],kgtext\n        return kgtext,full\n\n\n    def pred2(self, text,ner1,p):\n        # training_step defined the train loop.\n        # It is independent of forward\n        inputs = self.tokenizer([ner1+p+\"为\"+\"\".join([\"[MASK]\"]*10)],[text], return_tensors=\"pt\", padding=\"max_length\",max_length=128,truncation=True)\n        outputs = self(**inputs)\n        logits = outputs.logits\n        kgtext=[]\n        full=[]\n        words=tokenizer.tokenize(text)\n        for it in logits.argmax(dim=-1).tolist()[0]:\n#             print(tokenizer.decode(it))\n            full.append(tokenizer.decode(it))\n            if tokenizer.decode(it) in [\"[SEP]\",\"[PAD]\"]:\n                continue\n            \n            kgtext.append(tokenizer.decode(it))\n#         i=\"\".join(kgtext).index('关系为')\n#         return kgtext[i+3],kgtext\n        return kgtext,full\n#         x, token_type_ids,attention_mask = batch\n#         outputs = self(x, token_type_ids,attention_mask)\n#         self.log('test_loss', outputs.loss)\n#         return outputs.loss\n#     help(model.predict)\n    def configure_optimizers(self):\n            \"\"\"优化器 # 类似于余弦，但其周期是变化的，初始周期为T_0,而后周期会✖️T_mult。每个周期学习率由大变小； https://www.notion.so/62e72678923f4e8aa04b73dc3eefaf71\"\"\"\n    #         optimizer = torch.optim.AdamW(self.parameters(), lr=(self.learning_rate))\n\n            #只优化部分\n#             optimizer = torch.optim.AdamW(self.parameters(), lr=(self.hparams.learning_rate))\n            if self.hparams.optimizer_name==\"AdamW\":\n                optimizer = AdamW(self.parameters(), lr=(self.hparams.learning_rate))\n            else:\n                optimizer = getattr(optim, self.hparams.optimizer_name)(self.parameters(), lr=self.hparams.learning_rate)\n    # https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta\n#             optimizer = self.get_optimizer()\n            #         使用自适应调整模型\n            T_mult=2\n            scheduler =torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=self.hparams.T_max,T_mult=T_mult,eta_min=0 ,verbose=False)\n    #         https://github.com/PyTorchLightning/pytorch-lightning/blob/6dc1078822c33fa4710618dc2f03945123edecec/pytorch_lightning/core/lightning.py#L1119\n\n            lr_scheduler={\n    #            'optimizer': optimizer,\n               'scheduler': scheduler,\n#                 'reduce_on_plateau': True, # For ReduceLROnPlateau scheduler\n                'interval': 'step', #epoch/step\n                'frequency': 1,\n                'name':\"lr_scheduler\",\n                'monitor': 'train_loss', #监听数据变化\n                'strict': True,\n            }\n    #         return [optimizer], [lr_scheduler]\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.153225Z","iopub.execute_input":"2021-06-23T14:25:23.153463Z","iopub.status.idle":"2021-06-23T14:25:23.176280Z","shell.execute_reply.started":"2021-06-23T14:25:23.153440Z","shell.execute_reply":"2021-06-23T14:25:23.175416Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# help(DataLoader)\n\n# /kaggle/input/reformerchinesemodel/epoch4step21209.ckpt\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n# 自动停止\n# https://pytorch-lightning.readthedocs.io/en/1.2.1/common/early_stopping.html\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\n# 引入修剪技术　https://pytorch-lightning.readthedocs.io/en/stable/advanced/pruning_quantization.html\nfrom pytorch_lightning.callbacks import ModelPruning\nimport torch.nn.utils.prune as prune\n# https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html\n# 量化　降低内存　低精度　　https://pytorch-lightning.readthedocs.io/en/stable/advanced/pruning_quantization.html\nfrom pytorch_lightning.callbacks import QuantizationAwareTraining\n\n# 使用 DDP 时设置 find_unused_pa​​rameters=False\n# 默认情况下，我们已启用查找未使用的参数为 True。这是针对过去出现的兼容性问题（有关更多信息，请参阅讨论）。默认情况下，这会影响性能，并且在大多数情况下可以禁用。\nfrom pytorch_lightning.plugins import DDPPlugin\n\nfrom pytorch_lightning.loggers import NeptuneLogger\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.177552Z","iopub.execute_input":"2021-06-23T14:25:23.177945Z","iopub.status.idle":"2021-06-23T14:25:23.193498Z","shell.execute_reply.started":"2021-06-23T14:25:23.177908Z","shell.execute_reply":"2021-06-23T14:25:23.192735Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用小数据测试用","metadata":{}},{"cell_type":"code","source":"MAX=10000\n# 小数据截取测试用\ninput_ids,token_type_ids,attention_mask,labels=devdataset[:int(MAX*0.2)]\ndevdataset=TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n\n\ninput_ids,token_type_ids,attention_mask,labels=testdataset[:int(MAX*0.2)]\ntestdataset=TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n\n\ninput_ids,token_type_ids,attention_mask,labels=traindataset[:MAX]\ntraindataset=TensorDataset(input_ids,token_type_ids,attention_mask,labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:33:14.503444Z","iopub.execute_input":"2021-06-23T14:33:14.503876Z","iopub.status.idle":"2021-06-23T14:33:14.517449Z","shell.execute_reply.started":"2021-06-23T14:33:14.503837Z","shell.execute_reply":"2021-06-23T14:33:14.516390Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Problem finishing run\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 1502, in _atexit_cleanup\n    self._on_finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 1667, in _on_finish\n    self._poll_exit_response = self._wait_for_finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 1615, in _wait_for_finish\n    poll_exit_resp = self._backend.interface.communicate_poll_exit()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 759, in communicate_poll_exit\n    result = self._communicate(rec, timeout=timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 533, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 82, in get\n    is_set = self._object_ready.wait(timeout)\n  File \"/opt/conda/lib/python3.7/threading.py\", line 552, in wait\n    signaled = self._cond.wait(timeout)\n  File \"/opt/conda/lib/python3.7/threading.py\", line 296, in wait\n    waiter.acquire()\nException\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 1502, in _atexit_cleanup\n    self._on_finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 1667, in _on_finish\n    self._poll_exit_response = self._wait_for_finish()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 1615, in _wait_for_finish\n    poll_exit_resp = self._backend.interface.communicate_poll_exit()\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 759, in communicate_poll_exit\n    result = self._communicate(rec, timeout=timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 533, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 82, in get\n    is_set = self._object_ready.wait(timeout)\n  File \"/opt/conda/lib/python3.7/threading.py\", line 552, in wait\n    signaled = self._cond.wait(timeout)\n  File \"/opt/conda/lib/python3.7/threading.py\", line 296, in wait\n    waiter.acquire()\nException\n","output_type":"stream"}]},{"cell_type":"code","source":"  sweep_config = {\n    \"method\": \"bayes\",   # Random search https://docs.wandb.ai/guides/sweeps/configuration#search-strategy\n  \"metric\": {           # We want to maximize val_acc\n#       \"name\": \"val_acc\",\n#       \"goal\": \"maximize\"\n      \"name\": \"val_loss\",\n      \"goal\": \"minimize\",\n#       \"target\": 0.99\n  },\n    \n#   \"method\": \"random\",   # Random search https://docs.wandb.ai/guides/sweeps/configuration#search-strategy\n  \"parameters\": {\n        \"batch_size\":{\n            \"values\": [12,24]\n        },\n\n        \"optimizer_name\": {\n            # Choose from pre-defined values\n            \"values\": [\"AdamW\"]\n        },\n      \n         \"learning_rate\": {\n            # log uniform distribution between exp(min) and exp(max)\n            \"distribution\": \"log_uniform\",\n            \"min\": -9.21,   # exp(-9.21) = 1e-4\n            \"max\": -4.61    # exp(-4.61) = 1e-2\n        },     \n         \"accumulate_grad_batches\": {\n            # Choose from pre-defined values\n            \"values\": [1,2,3,4,5]\n        },     \n      \n\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:33:19.966955Z","iopub.execute_input":"2021-06-23T14:33:19.967286Z","iopub.status.idle":"2021-06-23T14:33:19.974611Z","shell.execute_reply.started":"2021-06-23T14:33:19.967256Z","shell.execute_reply":"2021-06-23T14:33:19.973730Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"optuna优化_百度数据做开放关系BD_KnowledgeExtractionnoteb\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:26:29.347417Z","iopub.execute_input":"2021-06-23T14:26:29.347788Z","iopub.status.idle":"2021-06-23T14:26:29.854001Z","shell.execute_reply.started":"2021-06-23T14:26:29.347757Z","shell.execute_reply":"2021-06-23T14:26:29.852590Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Create sweep with ID: r81jxsys\nSweep URL: https://wandb.ai/terrychanorg/optuna%E4%BC%98%E5%8C%96_%E7%99%BE%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%81%9A%E5%BC%80%E6%94%BE%E5%85%B3%E7%B3%BBBD_KnowledgeExtractionnoteb/sweeps/r81jxsys\n","output_type":"stream"}]},{"cell_type":"code","source":"def sweep_iteration():\n    # set up W&B logger\n    wandb.init()    # required to have access to `wandb.config`\n    wandb_logger = WandbLogger()\n    train_loader=DataLoader(traindataset,batch_size=wandb.config.batch_size, shuffle=True, )\n    val_loader=DataLoader(devdataset,batch_size=wandb.config.batch_size, shuffle=False, )\n#     test_loader=DataLoader(testdataset,batch_size=wandb.config.batch_size, shuffle=False, )\n    model=LitAutoLM(learning_rate=wandb.config.learning_rate,optimizer_name=wandb.config.optimizer_name)\n\n    early_stop_callback = EarlyStopping(\n       monitor='val_loss',\n       min_delta=0.000,\n       patience=20,\n       verbose=True,\n       mode='max'\n    )\n    lr_monitor = LearningRateMonitor(logging_interval='step')    \n    \n    \n    trainer = pl.Trainer(\n            gpus=1,\n        #     min_epochs=1,\n            precision=16,amp_level='O2',\n#             checkpoint_callback=checkpoint_callback,\n            callbacks=[],\n    #         resume_from_checkpoint=\"/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/百度数据做开放关系BD_Knowledge\\ Extractionnotebookb9f0237b84/3cu3oiy2/checkpoints/chinese-out.ckpt\",\n            auto_select_gpus=True,\n#             callbacks=[lr_monitor,early_stop_callback],\n            deterministic=True,\n    #         auto_scale_batch_size='binsearch',\n    #         auto_lr_find=True,\n    #         max_epochs=wandb.config.epochs,\n            max_epochs=5,\n            logger=wandb_logger,\n    #         accumulate_grad_batches=wandb.config.accumulate_grad_batches\n\n            accumulate_grad_batches=wandb.config.accumulate_grad_batches)\n    # train\n    trainer.fit(model,train_dataloader=train_loader,val_dataloaders=val_loader)\n\n    \ncount = 50 # number of runs to execute\nwandb.agent(sweep_id, function=sweep_iteration,count =count)\n# while not sweep.done():\n#     sweep.print_status()\n#     sweep.step()\n#     time.sleep(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:33:23.377276Z","iopub.execute_input":"2021-06-23T14:33:23.377628Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"624ee1f53dae4d0090ff150202b00b2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562f6da5a10845dbbefb028cb4a7e95d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe1328b18724c15bccccfabbb21dc41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d496835d5f924148af0f6d8da986ab39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32880e39773e4f8fb4ec18246075ae38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2e2fb677c94c4b97c4364464a994d7"}},"metadata":{}}]},{"cell_type":"code","source":"#     train_loader=DataLoader(traindataset,batch_size=batch_size, shuffle=True, )\n#     val_loader=DataLoader(devdataset,batch_size=batch_size, shuffle=False, )\n#     test_loader=DataLoader(testdataset,batch_size=batch_size, shuffle=False, )\n#     model=LitAutoLM(learning_rate=learning_rate,optimizer_name=optimizer_name)\n    \n#     trainer = pl.Trainer(\n#             gpus=1,\n#         #     min_epochs=1,\n#             precision=16,amp_level='O2',\n# #             checkpoint_callback=checkpoint_callback,\n#             callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n#     #         resume_from_checkpoint=\"/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/百度数据做开放关系BD_Knowledge\\ Extractionnotebookb9f0237b84/3cu3oiy2/checkpoints/chinese-out.ckpt\",\n#             auto_select_gpus=True,\n# #             callbacks=[lr_monitor,early_stop_callback],\n#             deterministic=True,\n#     #         auto_scale_batch_size='binsearch',\n#     #         auto_lr_find=True,\n#     #         max_epochs=wandb.config.epochs,\n#             max_epochs=1,\n#     #         logger=wandb_logger,\n#     #         accumulate_grad_batches=wandb.config.accumulate_grad_batches\n\n#             accumulate_grad_batches=2)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.230851Z","iopub.status.idle":"2021-06-23T14:25:23.231459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# optuna 优化\n\nhttps://optuna.readthedocs.io/zh_CN/latest/reference/generated/optuna.trial.Trial.html?highlight=suggest_float#optuna.trial.Trial.suggest_loguniform","metadata":{}},{"cell_type":"code","source":"# import logging,sys\n# def objective(trial: optuna.trial.Trial) -> float:\n#     # wandb_logger = WandbLogger(name='nerchaijie拆解',project=\"colab_optuna 使用pytorch_lightning超参优化 .ipynb\")\n#     # wandb.init()\n#     # name=str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n#     # name=str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))+str(num)\n#     # num=num+1\n#     # wandb_logger = WandbLogger(project=\"colab_optuna 使用pytorch_lightning超参优化 .ipynb\")\n#     # wandb_logger = WandbLogger()\n#     # We optimize the number of layers, hidden units in each layer and dropouts.\n# #     n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n#     learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n# #     max_epochs = trial.suggest_int(\"max_epochs\", 1, 1)\n#     batch_size = trial.suggest_categorical(\"batch_size\", [12])\n# # \"MomentumSGD\",\n# #     optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"AdamW\", \"RMSprop\", \"SGD\"])\n#     optimizer_name = trial.suggest_categorical(\"optimizer\", [\"AdamW\"])\n#     accumulate_grad_batches=trial.suggest_int(\"accumulate_grad_batches\", 1, 5)\n# #     batch_size=72\n\n\n#     train_loader=DataLoader(traindataset,batch_size=batch_size, shuffle=True, )\n#     val_loader=DataLoader(devdataset,batch_size=batch_size, shuffle=False, )\n#     test_loader=DataLoader(testdataset,batch_size=batch_size, shuffle=False, )\n#     model=LitAutoLM(learning_rate=learning_rate,optimizer_name=optimizer_name)\n    \n#     trainer = pl.Trainer(\n#             gpus=1,\n#         #     min_epochs=1,\n#             precision=16,amp_level='O2',\n# #             checkpoint_callback=checkpoint_callback,\n#             callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n#     #         resume_from_checkpoint=\"/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/百度数据做开放关系BD_Knowledge\\ Extractionnotebookb9f0237b84/3cu3oiy2/checkpoints/chinese-out.ckpt\",\n#             auto_select_gpus=True,\n# #             callbacks=[lr_monitor,early_stop_callback],\n#             deterministic=True,\n#     #         auto_scale_batch_size='binsearch',\n#     #         auto_lr_find=True,\n#     #         max_epochs=wandb.config.epochs,\n#             max_epochs=1,\n#     #         logger=wandb_logger,\n#     #         accumulate_grad_batches=wandb.config.accumulate_grad_batches\n\n#             accumulate_grad_batches=2)\n\n    \n#     hyperparameters = dict(learning_rate=learning_rate,batch_size=batch_size,optimizer_name=optimizer_name)\n#     print(\"hyperparameters\",hyperparameters)\n#     trainer.logger.log_hyperparams(hyperparameters)\n    \n#     trainer.fit(model, train_loader,val_loader)\n#     # still doesn't work\n#     val_loss=trainer.callback_metrics[\"val_loss\"].item()\n#     del model\n# #     del trainer\n#     torch.cuda.empty_cache()\n#     return val_loss\n\n\n\n# study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n# study.optimize(objective, n_trials=100, timeout=600)\n\n\n\n# print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n# print(\"Best trial:\")\n# trial = study.best_trial\n\n# print(\"  Value: {}\".format(trial.value))\n\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.232924Z","iopub.status.idle":"2021-06-23T14:25:23.233519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f\"Sampler is {study.sampler.__class__.__name__}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.234684Z","iopub.status.idle":"2021-06-23T14:25:23.235270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Best trial:\")\n# trial = study.best_trial\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.236686Z","iopub.status.idle":"2021-06-23T14:25:23.237252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 可视化\n\nhttps://optuna.readthedocs.io/zh_CN/latest/tutorial/10_key_features/005_visualization.html#sphx-glr-tutorial-10-key-features-005-visualization-py","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.238514Z","iopub.status.idle":"2021-06-23T14:25:23.239064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_parallel_coordinate(study)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.240396Z","iopub.status.idle":"2021-06-23T14:25:23.240996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 恢复大数据训练\n","metadata":{}},{"cell_type":"code","source":"devdataset=TensorDataset(dev['input_ids'],dev['token_type_ids'],dev['attention_mask'],dev['labels'])\ntestdataset=TensorDataset(test['input_ids'],test['token_type_ids'],test['attention_mask'],test['labels'])\ntraindataset=TensorDataset(train['input_ids'],train['token_type_ids'],train['attention_mask'],train['labels'])\n\n\n\nMAX=1000000000\n# 小数据截取测试用\ninput_ids,token_type_ids,attention_mask,labels=devdataset[:int(MAX*0.2)]\ndevdataset=TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n\n\ninput_ids,token_type_ids,attention_mask,labels=testdataset[:int(MAX*0.2)]\ntestdataset=TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n\n\ninput_ids,token_type_ids,attention_mask,labels=traindataset[:MAX]\ntraindataset=TensorDataset(input_ids,token_type_ids,attention_mask,labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.242543Z","iopub.status.idle":"2021-06-23T14:25:23.243224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Params: \")\n\nbest_params={}\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n    best_params[key]=value\n    \n\n\n\ntrain_loader=DataLoader(traindataset,batch_size=best_params[\"batch_size\"], shuffle=True, )\nval_loader=DataLoader(devdataset,batch_size=best_params[\"batch_size\"], shuffle=False, )\ntest_loader=DataLoader(testdataset,batch_size=best_params[\"batch_size\"], shuffle=False, )\nmodel=LitAutoLM(learning_rate=best_params[\"learning_rate\"],optimizer_name=best_params[\"optimizer\"])\n\nwandb_logger = WandbLogger(project=\"optuna优化_百度数据做开放关系BD_KnowledgeExtractionnoteb\")\n\nseed_everything(42)\n\nearly_stop_callback = EarlyStopping(\n   monitor='val_loss',\n   min_delta=0.0000,\n   patience=15,\n   verbose=True,\n   mode='min',\n#   check_finite=True,\n#  check_on_train_epoch_end=True,\n#     divergence_threshold=0.1\n    \n)\n\n!mkdir /kaggle/working/checkpoints\ncheckpoint_callback = ModelCheckpoint(\n#     filename='/kaggle/working/{epoch}-{val_loss:.2f}',\n    dirpath=\"/kaggle/working/checkpoints\",\n#     filename='{epoch:02d}-{step}-{val_loss:.2f}',\n    filename='out',\n#     save_last=True,\n    verbose=True,\n    monitor='val_loss',\n#     every_n_train_steps=2,\n    mode='min',\n#     best_model_path='best'\n    save_top_k=2\n)\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n\n\n\n\n\n\ntrainer = pl.Trainer(\n        gpus=1,\n    #     min_epochs=1,\n        precision=16,amp_level='O2',\n        val_check_interval=0.25, #这里增加检查验证的频率\n        limit_val_batches=0.25, # 限制验证的数目 降低每次验证的批次大小\n        checkpoint_callback=True,\n#             checkpoint_callback=checkpoint_callback,\n#         callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n        resume_from_checkpoint=\"../input/optuna-bd-knowledgeextractionnoteb/checkpoints/epoch=6-step=28455.ckpt\",\n        auto_select_gpus=True,\n \n        callbacks=[lr_monitor,early_stop_callback,\n#                pruning,\n               checkpoint_callback\n        #                        QuantizationAwareTraining()\n              ],\n \n#             callbacks=[lr_monitor,early_stop_callback],\n        deterministic=True,\n#         auto_scale_batch_size='binsearch',\n#         auto_lr_find=True,\n#         max_epochs=wandb.config.epochs,\n        max_epochs=10,\n        logger=wandb_logger,\n#         accumulate_grad_batches=wandb.config.accumulate_grad_batches\n#         terminate_on_nan=True, # 出现nan则停止\n        accumulate_grad_batches=2)\n\n\n\n# trainer.fit(model, train_loader,val_loader)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.244639Z","iopub.status.idle":"2021-06-23T14:25:23.245191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# lr_monitor = LearningRateMonitor(logging_interval='step')\n# early_stop_callback = EarlyStopping(\n#    monitor='train_loss',\n#    min_delta=0.00,\n#    patience=10,\n#    verbose=True,\n#    mode='min'\n# )\n# checkpoint_callback = ModelCheckpoint(\n#     filename='chinese-out',\n#     verbose=True,\n#     monitor='train_loss',\n#     mode='min',\n# #     best_model_path='best'\n#     save_top_k=2\n# )\n\n\n\n# model=LitAutoLM()\n# trainer = pl.Trainer(\n#         gpus=1,\n#     #     min_epochs=1,\n#         precision=16,amp_level='O2',\n#         checkpoint_callback=checkpoint_callback,\n# #         resume_from_checkpoint=\"/kaggle/input/openkgmodelml/wandb/run-20210610_054617-3cu3oiy2/files/百度数据做开放关系BD_Knowledge\\ Extractionnotebookb9f0237b84/3cu3oiy2/checkpoints/chinese-out.ckpt\",\n#         auto_select_gpus=True,\n#         callbacks=[lr_monitor,early_stop_callback],\n#         deterministic=True,\n# #         auto_scale_batch_size='binsearch',\n# #         auto_lr_find=True,\n# #         max_epochs=wandb.config.epochs,\n#         max_epochs=5,\n# #         logger=wandb_logger,\n# #         accumulate_grad_batches=wandb.config.accumulate_grad_batches\n\n#         accumulate_grad_batches=2)\n\n# trainer.fit(model, train_loader,val_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.246605Z","iopub.status.idle":"2021-06-23T14:25:23.247155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import pipeline\n# # https://huggingface.co/uer/roberta-base-word-chinese-cluecorpussmall\n# # unmasker = pipeline('fill-mask', model='uer/roberta-medium-word-chinese-cluecorpussmall')\n# unmasker = pipeline('fill-mask', model='uer/roberta-base-word-chinese-cluecorpussmall')\n# unmasker(\"如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈 [SEP] 喜剧之王和周星驰关系为 [MASK]\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.248413Z","iopub.status.idle":"2021-06-23T14:25:23.248965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainer.test(test_dataloaders=test_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.250299Z","iopub.status.idle":"2021-06-23T14:25:23.250847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dir(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.252062Z","iopub.status.idle":"2021-06-23T14:25:23.252643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntorch.save(model.state_dict(), \"model.bin\")\n\n# model = TheModelClass(*args, **kwargs)\n# model.load_state_dict(torch.load(PATH))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.253921Z","iopub.status.idle":"2021-06-23T14:25:23.254487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=LitAutoLM()\nmodel.load_state_dict(torch.load(\"model.bin\"))\n# model.load_state_dict(torch.load(\"../input/openkgmodelml/model.bin\"))\n\n\nmodel.eval()\npass","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.255751Z","iopub.status.idle":"2021-06-23T14:25:23.256303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nout=model.pred(\"硫磺溪是一条位于美国加利福尼亚州橙县的溪流，亦是亚里索溪的支流。长约7.7公里，流域面积约16平方公里。\",\"橙县\",\"美国加利福尼亚州\")\nprint(out)\n\nindex=\"\".join(out[0]).index('关系为')\n\npredicate=\"\".join(out[0])[index+3:]\n\n\nprint(predicate)\n\n\nout=model.pred2(\"硫磺溪是一条位于美国加利福尼亚州橙县的溪流，亦是亚里索溪的支流。长约7.7公里，流域面积约16平方公里。\",\"硫磺溪\",\"位于\")\nprint(out)\n\n# index=\"\".join(out[0]).index('关系为')\n\n# predicate=\"\".join(out[0])[index+3:]\n\n\n# print(predicate)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.257689Z","iopub.status.idle":"2021-06-23T14:25:23.258240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 测试模型效果","metadata":{}},{"cell_type":"code","source":"tokenizer = AlbertTokenizer.from_pretrained(\"uer/roberta-base-word-chinese-cluecorpussmall\")\n# model = BertForMaskedLM.from_pretrained(\"uer/roberta-base-word-chinese-cluecorpussmall\")\n\ninputs = tokenizer([\"如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈 \"],[\" 喜剧之王和周星驰关系为 [MASK] [MASK] \"], return_tensors=\"pt\", padding=\"max_length\",max_length=128,truncation=True)\nlabels = tokenizer([\"如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈 \"],[\" 喜剧之王和周星驰关系为主演\"], return_tensors=\"pt\", padding=\"max_length\",max_length=128,truncation=True)[\"input_ids\"]\n\noutputs = model(**inputs, labels=labels)\nloss = outputs.loss\nlogits = outputs.logits\n\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.259524Z","iopub.status.idle":"2021-06-23T14:25:23.260070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.261425Z","iopub.status.idle":"2021-06-23T14:25:23.261984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for it in logits.argmax(dim=-1).tolist()[0]:\n    print(tokenizer.decode(it))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:25:23.263256Z","iopub.status.idle":"2021-06-23T14:25:23.263808Z"},"trusted":true},"execution_count":null,"outputs":[]}]}